{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ancilary Functions:\n",
    "We will start by writing support functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Computes the softmax vector of the input\n",
    "    \n",
    "    Inputs:\n",
    "    x = np.array of shape (n_classes, n_samples)\n",
    "    \n",
    "    Outputs:\n",
    "    y = np.array of shape (n_classes, n_samples)\n",
    "    '''\n",
    "    \n",
    "    x_exp = np.exp(Z)\n",
    "    sum_x_exp = np.sum(x_exp, axis=0, keepdims=True)\n",
    "    assert(sum_x_exp.shape == (1, Z.shape[1]))\n",
    "    \n",
    "    S = x_exp / sum_x_exp\n",
    "    assert(S.shape == Z.shape)\n",
    "    cache = Z\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s, cache = softmax(Z)\n",
    "    #print('Shape of Z: ' + str(Z.shape))\n",
    "    #print('Shape of dA: ' + str(dA.shape))\n",
    "    #print('Type of s: ' + str(type(s)))\n",
    "    #print('Shape of s: ' + str(s.shape))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    \n",
    "    assert(AL.shape == Y.shape)\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    J = np.log(np.sum(AL*Y, axis=0, keepdims=True))\n",
    "    cost = (-1 / m) * np.sum(J)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(layer_dims):\n",
    "    \n",
    "    #np.random.seed(2)\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        params['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2/layer_dims[i])\n",
    "        params['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    Computes linear finction of the parameters W and b, and preveous activation A\n",
    "    \n",
    "    Inputs:\n",
    "    A = activation from the previous layer, A.shape = (n_(l-1), m)\n",
    "    W = parameters of layer l, W.shape = (n_l, n_(l-1))\n",
    "    b = bias of layer l, b.shape = (n_l, 1)\n",
    "    \n",
    "    Outputs:\n",
    "    Z = computed linear function of the node, Z.shape = (n_l, m)\n",
    "    cache = variable that stores parameters A_prev, Wl, bl to be used in back propargation\n",
    "    '''\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    assert(dA.shape == Z)\n",
    "    \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        A, activ_cache = relu(Z)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A, activ_cache = sigmoid(Z)\n",
    "    \n",
    "    if activation == 'softmax':\n",
    "        A, activ_cache = softmax(Z)\n",
    "        \n",
    "        \n",
    "    cache = lin_cache, activ_cache # cache = ((A_prev, W, b), Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_backward(dA, cache, activation):\n",
    "    \n",
    "    lin_cache, activ_cache = cache # lin_cache = (A, W, b), activ_cache = Z\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activ_cache)\n",
    "        \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activ_cache)\n",
    "    \n",
    "    if activation == 'softmax':\n",
    "        dZ = softmax_backward(dA, activ_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_forward(X, parameters):\n",
    "    \n",
    "    caches = []\n",
    "    n_layers = len(parameters)//2\n",
    "    \n",
    "    A_prev = X\n",
    "    \n",
    "    for layer in range(1, n_layers):\n",
    "        W = parameters['W' + str(layer)]\n",
    "        b = parameters['b' + str(layer)]\n",
    "        Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "        A, activ_cache = relu(Z)\n",
    "        \n",
    "        A_prev = A\n",
    "        \n",
    "        caches.append((lin_cache, activ_cache))\n",
    "    \n",
    "    WL = parameters['W' + str(n_layers)]\n",
    "    bL = parameters['b' + str(n_layers)]\n",
    "    ZL, lin_cache = linear_forward(A_prev, WL, bL)\n",
    "    AL, activ_cache = softmax(ZL) # \n",
    "    \n",
    "    caches.append((lin_cache, activ_cache))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # number of layers\n",
    "    m = Y.shape[1] # number of samples\n",
    "    \n",
    "    # compute the output vector from the model:\n",
    "    #S = softmax(AL)\n",
    "    #assert(S.shape == Y.shape)\n",
    "    \n",
    "    # initialize back propagation:\n",
    "    dAL = (-1/m) * np.divide(Y, AL)  # * S * (1 - S)\n",
    "    \n",
    "    #print('in L_backward, type/shape of dAL is: ' + str(type(dAL)) + ' ' + str(dAL.shape))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev, dW, db = activation_backward(dAL, current_cache, activation='softmax')\n",
    "    grads['dW' + str(L)] = dW\n",
    "    grads['db' + str(L)] = db\n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "        current_cache = caches[i]\n",
    "        dA_prev, dW, db = activation_backward(dA_prev, current_cache, activation='relu')\n",
    "        grads['dW' + str(i+1)] = dW\n",
    "        grads['db' + str(i+1)] = db\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i + 1)] = parameters['W' + str(i + 1)] - learning_rate * grads['dW' + str(i + 1)]\n",
    "        parameters['b' + str(i + 1)] = parameters['b' + str(i + 1)] - learning_rate * grads['db' + str(i + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    AL, caches = L_forward(X, parameters)\n",
    "    \n",
    "    S = AL\n",
    "    \n",
    "    max_prob = np.argmax(S, axis=0)\n",
    "    \n",
    "    #print(max_prob.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        S[:, i] = 0\n",
    "        S[max_prob[i], i] = 1\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate =0.05, num_iterations=60, print_cost=True):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    parameters = init_params(layer_dims)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "    \n",
    "        # forward pass:\n",
    "        AL, caches = L_forward(X, parameters)\n",
    "\n",
    "        # calculate the cost:\n",
    "        #S = softmax(AL)\n",
    "        cost = cost_function(AL, Y)\n",
    "        costs.append(cost)\n",
    "\n",
    "        # backward pass:\n",
    "        grads = L_backward(AL, Y, caches)\n",
    "        \n",
    "        ### THIS IS WHERE GRADIENT CHECKING WOULD BE\n",
    "        \n",
    "        # update parameters:\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # print the cost:\n",
    "        if print_cost and any([(i+1)%100==0, i == 0]):\n",
    "            print('Cost after ' + str(i + 1) + ' iterations is ' + str(cost) + ' at ' + str(datetime.now()))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can train our model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load('train_x.npy')\n",
    "Y = np.load('train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "mu = (1/m) * np.sum(X, axis=1, keepdims=True)\n",
    "X = X - mu\n",
    "\n",
    "sigma = (1/m) * np.sum(X**2, axis=1, keepdims=True)\n",
    "\n",
    "X = X/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dims = (54, 70, 50, 30, 20, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = L_layer_model(X, Y, layer_dims, learning_rate =0.1, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(Y_hat, Y):\n",
    "    \n",
    "    assert(Y_hat.shape == Y.shape)\n",
    "    \n",
    "    m = Y.shape[1] # number of data entries\n",
    "    \n",
    "    w = 0\n",
    "    c = 0\n",
    "    \n",
    "    A = Y_hat.T\n",
    "    B = Y.T\n",
    "    assert(A.shape == B.shape)\n",
    "    for i in range(Y.shape[1]):\n",
    "        if np.all(A[i] == B[i]):\n",
    "            c += 1\n",
    "        else:\n",
    "            w += 1\n",
    "    \n",
    "    return c/m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = predict(X, params)\n",
    "out = out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(out, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test the neuronex on a different set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load('abc_train_X.npy')\n",
    "Y = np.load('abc_train_Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = np.zeros((2, 300))\n",
    "\n",
    "for i in range(300):\n",
    "    if Y[0, i] == 1:\n",
    "        Z[1, i] = 1\n",
    "    else:\n",
    "        Z[0, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dims = (2, 10, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = L_layer_model(X, Z, layer_dims, learning_rate =0.01, num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
